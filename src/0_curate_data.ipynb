{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Curate Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw data\n",
    "\n",
    "At this point, you have already run `make pull-data`. This should have downloaded and preprocessed the data into individual files. Your file structure should look like this:\n",
    "\n",
    "```\n",
    "data/\n",
    "├── processed\n",
    "│   ├── countries.txt\n",
    "│   ├── english.txt\n",
    "│   ├── french.txt\n",
    "│   └── names.txt\n",
    "└── raw\n",
    "    ├── data.zip\n",
    "    ├── eng-fra\n",
    "    │   └── eng-fra.txt\n",
    "    └── names\n",
    "        ├── Arabic.txt\n",
    "        ├── Chinese.txt\n",
    "        ├── Czech.txt\n",
    "        ├── Dutch.txt\n",
    "        ├── English.txt\n",
    "        ├── French.txt\n",
    "        ├── German.txt\n",
    "        ├── Greek.txt\n",
    "        ├── Irish.txt\n",
    "        ├── Italian.txt\n",
    "        ├── Japanese.txt\n",
    "        ├── Korean.txt\n",
    "        ├── Polish.txt\n",
    "        ├── Portuguese.txt\n",
    "        ├── Russian.txt\n",
    "        ├── Scottish.txt\n",
    "        ├── Spanish.txt\n",
    "        └── Vietnamese.txt\n",
    "```\n",
    "\n",
    "The 'names' and 'countries' data correspond to this pytorch tutorial: [https://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html](https://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html)\n",
    "\n",
    "\n",
    "The 'english' and 'french' data correspond to this one: [https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html)\n",
    "\n",
    "\n",
    "In this notebook, we will use our Language utility class to scan these corpuses (corpa?) and encode the words/sentences into indices of tokens in a vocabulary. \n",
    "\n",
    "\n",
    "### Processing Strategy\n",
    "\n",
    "1. Take a text file where each line is an observation of a sentence in a given language.\n",
    "\n",
    "2. Tokenize this sentence into tokens. Could be a basic tokenization such as splitting a sentence on spaces into component words; or more sophisticated tokenization, like one of the spaCy models.\n",
    "\n",
    "3. Scan over all the tokens in all the data in the file, collect a vocabulary. \n",
    "\n",
    "4. If necessary, add special tokens to the vocabulary for padding, unknown token, start/end of sentence.\n",
    "\n",
    "5. Write a new file where each line gets tokenized and converted into the indices of those tokens in the vocabulary.\n",
    "\n",
    "6. Save the model so we can load it and access its vocab/tokenizer later.\n",
    "\n",
    "\n",
    "For example, we have an input file like this:\n",
    "\n",
    "```\n",
    "names.txt\n",
    "\n",
    "Adam\n",
    "Ahearn\n",
    "Aodh\n",
    "Aodha\n",
    "```\n",
    "\n",
    "and we create a language model that tokenizes it by splitting the name into component letters:\n",
    "\n",
    "```\n",
    "names_indices.txt\n",
    "\n",
    "2 23 18 4 19 3\n",
    "2 23 10 7 4 9 8 3\n",
    "2 23 5 18 10 3\n",
    "2 23 5 18 10 4 3\n",
    "```\n",
    "\n",
    "Here special tokens have been added, so that the name always begins with token 2 (<SOS>) and ends with token 3 (<EOS>)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "\n",
    "from common.language import Language\n",
    "\n",
    "from common.utils import get_logger\n",
    "\n",
    "logger = get_logger(\"curation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-15 14:48:46 - curation - INFO: Current working directory: /home/rob/encoder-decoder\n"
     ]
    }
   ],
   "source": [
    "# Set the cwd to the root of the project.\n",
    "# Only let this execute once\n",
    "if os.getcwd().endswith(\"src\"):\n",
    "    os.chdir(\"..\")\n",
    "\n",
    "logger.info(f\"Current working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config.yaml. This contains all of our paths and constants.\n",
    "with open(\"config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split names into component letters\n",
    "\n",
    "In the RNN tutorial, the input sequence is each name letter by letter, and the target output is the country that the name originates from. \n",
    "\n",
    "Split the names up by letter, encode them. Treate the country name as a whole word, encode it.\n",
    "\n",
    "In our second exercise, we'll train an encoder-decoder to spell out the country name, so we want a version of this that has the letters encoded individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have two types of tokenizer here:\n",
    "#   When given a string, split into individual characters (convert to list)\n",
    "#   When given a string, split into individual words (wrap in list)\n",
    "names = Language(\n",
    "    name=\"names\",\n",
    "    tokenizer_name=\"split_into_chars\", \n",
    "    detokenizer_name=\"join_no_space\",\n",
    "    )\n",
    "\n",
    "# Since this is a classification task, we don't need to add <SOS> or <EOS>\n",
    "# tokens. Keep whole words.\n",
    "countries_word = Language(\n",
    "    name=\"countries_word\",\n",
    "    tokenizer_name=\"keep_whole_sentence\",\n",
    "    detokenizer_name=\"join_no_space\",\n",
    "    add_special_tokens=False,\n",
    ")\n",
    "\n",
    "# Split into individual characters\n",
    "countries_letters = Language(\n",
    "    name=\"countries_letters\",\n",
    "    tokenizer_name=\"split_into_chars\",\n",
    "    detokenizer_name=\"join_no_space\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "names.scan_corpus(config[\"NAMES_INPUT_PATH\"])\n",
    "countries_word.scan_corpus(config[\"COUNTRIES_INPUT_PATH\"])\n",
    "countries_letters.scan_corpus(config[\"COUNTRIES_INPUT_PATH\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of names vocab:  91\n",
      "Length of countries vocab:  19\n",
      "Length of countries as letters vocab:  35\n",
      "\n",
      "Example elements: \n",
      "\n",
      "['<PAD>', '<UNK>', '<SOS>', '<EOS>', 'a', 'o', 'i', 'e', 'n', 'r']\n",
      "['Russian', 'English', 'Arabic', 'Japanese', 'German', 'Italian', 'Czech', 'Spanish', 'Dutch', 'French']\n",
      "['<PAD>', '<UNK>', '<SOS>', '<EOS>', 'i', 'n', 's', 'a', 'u', 'R']\n"
     ]
    }
   ],
   "source": [
    "# Vocab lengths\n",
    "print(\"Length of names vocab: \", len(names.vocabulary))\n",
    "print(\"Length of countries vocab: \", len(countries_word.vocabulary))\n",
    "print(\n",
    "    \"Length of countries as letters vocab: \", len(countries_letters.vocabulary)\n",
    ")\n",
    "\n",
    "print(\"\\nExample elements: \\n\")\n",
    "print(list(names.vocabulary)[0:10])\n",
    "print(list(countries_word.vocabulary)[0:10])\n",
    "print(list(countries_letters.vocabulary)[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that our tokenizers have not converted to lowercase first. We're leaving capital letters in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert each line to a list of indices and save to a new file\n",
    "names.convert_corpus_to_indices(\n",
    "    config[\"NAMES_INPUT_PATH\"],\n",
    "    config[\"NAMES_OUTPUT_PATH\"],\n",
    ")\n",
    "\n",
    "countries_word.convert_corpus_to_indices(\n",
    "    config[\"COUNTRIES_INPUT_PATH\"],\n",
    "    config[\"COUNTRIES_WORD_OUTPUT_PATH\"],\n",
    ")\n",
    "\n",
    "countries_letters.convert_corpus_to_indices(\n",
    "    config[\"COUNTRIES_INPUT_PATH\"],\n",
    "    config[\"COUNTRIES_LETTER_OUTPUT_PATH\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the language models\n",
    "names.save(config[\"NAMES_LANGUAGE_MODEL_PATH\"])\n",
    "\n",
    "countries_word.save(config[\"COUNTRIES_WORD_LANGUAGE_MODEL_PATH\"])\n",
    "\n",
    "countries_letters.save(config[\"COUNTRIES_LETTER_LANGUAGE_MODEL_PATH\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English and French"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If name is english or french, will use a default tokenizer\n",
    "english = Language(\n",
    "    name=\"english\",\n",
    "    tokenizer_name=\"spacy_english\",\n",
    "    detokenizer_name=\"join_with_space\",\n",
    ")\n",
    "\n",
    "french = Language(\n",
    "    name=\"french\",\n",
    "    tokenizer_name=\"spacy_french\",\n",
    "    detokenizer_name=\"join_with_space\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scan to learn vocabulary. We're going to limit the vocab size to 10000 tokens.\n",
    "english.scan_corpus(\n",
    "    config[\"ENGLISH_INPUT_PATH\"],\n",
    "    max_vocab_size=7500,\n",
    ")\n",
    "\n",
    "french.scan_corpus(\n",
    "    config[\"FRENCH_INPUT_PATH\"],\n",
    "    max_vocab_size=7500,\n",
    ")\n",
    "\n",
    "# Save models\n",
    "english.save(config[\"ENGLISH_LANGUAGE_MODEL_PATH\"])\n",
    "french.save(config[\"FRENCH_LANGUAGE_MODEL_PATH\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<PAD>', '<UNK>', '<SOS>', '<EOS>', '.', 'I', 'you', 'to', '?', 'the']\n",
      "['<PAD>', '<UNK>', '<SOS>', '<EOS>', '.', 'Je', 'de', '?', 'pas', 'est']\n"
     ]
    }
   ],
   "source": [
    "# Top vocab\n",
    "print(list(english.vocabulary)[0:10])\n",
    "print(list(french.vocabulary)[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['<SOS>', 'Hello', 'there', ',', 'skibidi', 'yeet', '<EOS>']\n",
      "Indices: [2, 3861, 85, 24, 1, 1, 3]\n",
      "Tokens: ['<SOS>', 'Hello', 'there', ',', '<UNK>', '<UNK>', '<EOS>']\n"
     ]
    }
   ],
   "source": [
    "# Give example encodings\n",
    "\n",
    "tokens = english.tokenizer(\"Hello there, skibidi yeet\")\n",
    "# Add our special tokens\n",
    "tokens = [\"<SOS>\"] + tokens + [\"<EOS>\"]\n",
    "print(f\"Tokens: {tokens}\")\n",
    "\n",
    "indices = english.token_to_index(tokens)\n",
    "print(f\"Indices: {indices}\")\n",
    "\n",
    "# Inverse operation\n",
    "tokens = english.index_to_token(indices)\n",
    "print(f\"Tokens: {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will take a hot minute (about 5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-15 14:50:08 - curation - INFO: Converted English corpus to indices\n",
      "2024-10-15 14:51:34 - curation - INFO: Converted French corpus to indices\n"
     ]
    }
   ],
   "source": [
    "# Note that in conversion, the special tokens are added to front and back\n",
    "english.convert_corpus_to_indices(\n",
    "    config[\"ENGLISH_INPUT_PATH\"],\n",
    "    config[\"ENGLISH_OUTPUT_PATH\"],\n",
    ")\n",
    "logger.info(\"Converted English corpus to indices\")\n",
    "\n",
    "french.convert_corpus_to_indices(\n",
    "    config[\"FRENCH_INPUT_PATH\"],\n",
    "    config[\"FRENCH_OUTPUT_PATH\"],\n",
    ")\n",
    "logger.info(\"Converted French corpus to indices\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
