{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name Encoder-Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the same exercise again, but now instead of an RNN whose output layer gives the (log) probability that a name belongs to a particular country, let's have an encoder-decoder spell out the name of the country!\n",
    "\n",
    "We'll use the same names data set, where each name has been tokenized, converted into a vocabulary index, and sandwiched in <SOS>/<EOS> tokens. But now we'll use a verision of the country labels where they have undergone the same processing, rather than being converted to an index of a one-hot encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import yaml\n",
    "\n",
    "import torch\n",
    "import lightning as L\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from pytorch_lightning import seed_everything\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.utilities.model_summary import ModelSummary\n",
    "from lightning.pytorch.callbacks import LearningRateMonitor\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from common.language import load_language\n",
    "\n",
    "from common.dataloaders import (\n",
    "    TranslationDataset,\n",
    "    padding_collator,\n",
    ")\n",
    "\n",
    "from common.models import (\n",
    "    EncoderDecoder,\n",
    ")\n",
    "\n",
    "from common.utils import (\n",
    "    get_logger,\n",
    "    Timer,\n",
    ")\n",
    "\n",
    "logger = get_logger(\"name_classification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(2718)\n",
    "\n",
    "# Set the cwd to the root of the project.\n",
    "# Only let this execute once\n",
    "if os.getcwd().endswith(\"src\"):\n",
    "    os.chdir(\"..\")\n",
    "\n",
    "logger.info(f\"Current working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config.yaml. This contains all of our paths and constants.\n",
    "with open(\"config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Training params\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 128\n",
    "EMBEDDING_SIZE = 512\n",
    "HIDDEN_SIZE = 512\n",
    "\n",
    "# No unit type option here, just using GRUs.\n",
    "\n",
    "# Trainer params\n",
    "ACCELERATOR = \"gpu\"  # \"cpu\" or \"gpu\"\n",
    "\n",
    "# CPUS to give each dataloader\n",
    "NUM_WORKERS = 3\n",
    "\n",
    "# Every time you run training, the logs will have this tag attached.\n",
    "# If you rerun with the same tag, the log will be overwritten.\n",
    "TAG = (\n",
    "    f\"encoder-decoder_\"\n",
    "    f\"BATCH_SIZE={BATCH_SIZE}_\"\n",
    "    f\"EMBEDDING_SIZE={EMBEDDING_SIZE}_\"\n",
    "    f\"HIDDEN_SIZE={HIDDEN_SIZE}_\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same names language model\n",
    "names_language = load_language(config[\"NAMES_LANGUAGE_MODEL_PATH\"])\n",
    "\n",
    "# Now use the language model that tokenizes the country as letters and adds a start and end token\n",
    "countries_language = load_language(\n",
    "    config[\"COUNTRIES_LETTER_LANGUAGE_MODEL_PATH\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAMES_VOCAB_SIZE = len(names_language.vocabulary)\n",
    "COUNTRIES_VOCAB_SIZE = len(countries_language.vocabulary)\n",
    "\n",
    "print(f\"Names vocab size: {NAMES_VOCAB_SIZE}\")\n",
    "print(f\"Countries vocab size: {COUNTRIES_VOCAB_SIZE}\")\n",
    "\n",
    "# Vocabulary attributes are a dictionary with the token being the\n",
    "# key and the index being how frequently the token appears in the corpus.\n",
    "# Note that since we've added the special tokens ourselves, they will\n",
    "# have frequency 0.\n",
    "name_vocab = list(names_language.vocabulary.keys())\n",
    "country_vocab = list(countries_language.vocabulary.keys())\n",
    "\n",
    "print(\"Top 10 most common tokens in names vocabulary:\")\n",
    "for i in range(10):\n",
    "    print(f\"{name_vocab[i]}: {names_language.vocabulary[name_vocab[i]]}\")\n",
    "\n",
    "print(\"\\nTop 10 most common tokens in countries vocabulary:\")\n",
    "for i in range(10):\n",
    "    print(\n",
    "        f\"{country_vocab[i]}: {countries_language.vocabulary[country_vocab[i]]}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TranslationDataset(\n",
    "    source_labels_path=config[\"NAMES_INPUT_PATH\"],\n",
    "    target_labels_path=config[\"COUNTRIES_INPUT_PATH\"],\n",
    "    source_indices_path=config[\"NAMES_OUTPUT_PATH\"],\n",
    "    target_indices_path=config[\"COUNTRIES_LETTER_OUTPUT_PATH\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show random example\n",
    "x, y, name, country = dataset[np.random.randint(0, len(dataset))]\n",
    "print(\"Input: \", x)\n",
    "print(\"Target: \", y)\n",
    "print(\"Name: \", name)\n",
    "print(\"Country: \", country)\n",
    "\n",
    "# Note that both source and target languages have <SOS> and  <EOS> tokens now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data = torch.utils.data.random_split(dataset, [0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataloaders\n",
    "# We use a collate function to pad the sequences to the same length.\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    collate_fn=padding_collator,\n",
    ")\n",
    "\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    val_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    collate_fn=padding_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the data loader.\n",
    "\n",
    "x, y, names, countries = next(iter(train_dataloader))\n",
    "print(\"Source shape: \", x.shape)\n",
    "print(\"Target shape: \", y.shape)\n",
    "\n",
    "\n",
    "# Just to be sure, detokenize the first row\n",
    "print(\"\\nDetokenize the first row\")\n",
    "print(\"Source: \", names_language.index_to_token(x[0]))\n",
    "print(\"Target: \", countries_language.index_to_token(y[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EncoderDecoder(\n",
    "    source_language=names_language,\n",
    "    target_language=countries_language,\n",
    "    detokenizer=lambda s: \"\".join(s),\n",
    "    input_size=NAMES_VOCAB_SIZE,\n",
    "    output_size=COUNTRIES_VOCAB_SIZE,\n",
    "    embedding_size=EMBEDDING_SIZE,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    data_length=len(train_data),\n",
    "    max_output_length=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelSummary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that we can forward pass with the x,y generated above.\n",
    "\n",
    "context_vector = model.encoder_step(x)\n",
    "print(f\"Context vector shape: {context_vector.shape}\")\n",
    "\n",
    "output, hidden = model.decoder_step(y[:, :-1], context_vector, context_vector)\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Hidden shape: {hidden.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print how many parameters the model has\n",
    "print(\n",
    "    f\"Model has {sum(p.numel() for p in model.parameters() if p.requires_grad):,} parameters.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training logs\n",
    "\n",
    "tensorboard_logger = TensorBoardLogger(\n",
    "    save_dir=config[\"TENSORBOARD_LOGS_PATH\"],\n",
    "    name=\"name-encoder-decoder/\",\n",
    "    version=TAG,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use our nominated accelerator and log to tensorboard\n",
    "trainer = L.Trainer(\n",
    "    devices=1,\n",
    "    accelerator=ACCELERATOR,\n",
    "    logger=tensorboard_logger,\n",
    "    max_epochs=EPOCHS,\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor=\"validation_loss\", patience=3, mode=\"min\"),\n",
    "        LearningRateMonitor(logging_interval=\"step\"),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timer = Timer()\n",
    "trainer.fit(model, train_dataloader, val_dataloader)\n",
    "print(f\"Elapsed time: {timer.toc()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), config[\"NAME_ENCODER_DECODER_TORCH_MODEL_PATH\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.freeze()\n",
    "\n",
    "names = [\n",
    "    \"Thompson\",\n",
    "    \"Jackson\",\n",
    "    \"Satoshi\",\n",
    "    \"Javier\",\n",
    "    \"Wang\",\n",
    "    \"Kim\",\n",
    "    \"Lee\",\n",
    "    \"Kumar\",\n",
    "    \"Ericsson\",\n",
    "]\n",
    "\n",
    "for name in names:\n",
    "    print(f\"Name: {name} \\t Prediction: {model.inference(name)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty good! Note that Indian/Nordic names don't exist in the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate accuracy:\n",
    "model.eval()\n",
    "names = []\n",
    "actual = []\n",
    "predicted = []\n",
    "\n",
    "for x, y, name, country in val_data:\n",
    "    prediction = model.inference(name)\n",
    "\n",
    "    names.append(name)\n",
    "    actual.append(country)\n",
    "    predicted.append(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(actual, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = sorted(list(set(actual).union(set(predicted))))\n",
    "cm = confusion_matrix(actual, predicted, labels=labels, normalize=\"true\")\n",
    "cm = np.round(cm, 2)\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "ax = plt.gca()\n",
    "\n",
    "ConfusionMatrixDisplay(cm, display_labels=labels).plot(ax=ax)\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
