{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name Encoder-Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try an encoder decoder. Rather than classifying the country, let's encode the name; and feed a decoder a '<sos>' token and see if it can generate the country sequentially."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from common.utils import (\n",
    "    load_language,\n",
    ")\n",
    "\n",
    "import torch\n",
    "import lightning as L\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from pytorch_lightning import seed_everything\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 2718\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2718"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_everything(2718)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "ENGLISH_INPUT_PATH = \"../data/processed/english.txt\"\n",
    "ENGLISH_INDICES_INPUT_PATH = \"../data/processed/english_indices.txt\"\n",
    "FRENCH_INPUT_PATH = \"../data/processed/french.txt\"\n",
    "FRENCH_INDICES_INPUT_PATH = \"../data/processed/french_indices.txt\"\n",
    "\n",
    "# Language model paths\n",
    "ENGLISH_MODEL_PATH = \"../models/english_model.pkl\"\n",
    "FRENCH_MODEL_PATH = \"../models/french_model.pkl\"\n",
    "\n",
    "\n",
    "# Training params\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 128\n",
    "EMBEDDING_SIZE = 128\n",
    "HIDDEN_SIZE = 256\n",
    "UNIT_TYPE = \"GRU\"\n",
    "ACCELERATOR = \"gpu\"\n",
    "\n",
    "# CPUS to give each dataloader\n",
    "NUM_WORKERS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:common.utils:Loading from ../models/english_model.pkl\n",
      "INFO:common.utils:Creating a language object for english\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m01\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:common.utils:Loading from ../models/french_model.pkl\n",
      "INFO:common.utils:Creating a language object for french\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fr-core-news-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.8.0/fr_core_news_sm-3.8.0-py3-none-any.whl (16.3 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('fr_core_news_sm')\n",
      "English vocab size: 7504\n",
      "French vocab size: 7504\n"
     ]
    }
   ],
   "source": [
    "# Load the language models\n",
    "source_language = load_language(ENGLISH_MODEL_PATH)\n",
    "target_language = load_language(FRENCH_MODEL_PATH)\n",
    "\n",
    "SOURCE_VOCAB_SIZE = len(source_language.vocabulary)\n",
    "TARGET_VOCAB_SIZE = len(target_language.vocabulary)\n",
    "\n",
    "print(f\"English vocab size: {SOURCE_VOCAB_SIZE}\")\n",
    "print(f\"French vocab size: {TARGET_VOCAB_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageTranslationDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for the names dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        source_label_path,\n",
    "        target_label_path,\n",
    "        source_indices_path,\n",
    "        target_indices_path,\n",
    "    ):\n",
    "        self.source_label_path = source_label_path\n",
    "        self.target_label_path = target_label_path\n",
    "        self.source_indices_path = source_indices_path\n",
    "        self.target_indices_path = target_indices_path\n",
    "\n",
    "        self.source = []\n",
    "        self.target = []\n",
    "        self.source_indices = []\n",
    "        self.target_indices = []\n",
    "\n",
    "        # Load files\n",
    "        self._load_data()\n",
    "\n",
    "        return None\n",
    "\n",
    "    def _load_label_data(self, path):\n",
    "        \"\"\"\n",
    "        Load in a file where each line is a sentence.\n",
    "        \"\"\"\n",
    "        with open(path, \"r\") as f:\n",
    "            data = f.readlines()\n",
    "\n",
    "        data = [x.strip() for x in data]\n",
    "\n",
    "        return data\n",
    "\n",
    "    def _load_index_data(self, path):\n",
    "        \"\"\"\n",
    "        Load in a file where each line is a list of indices.\n",
    "        \"\"\"\n",
    "        with open(path, \"r\") as f:\n",
    "            data = f.readlines()\n",
    "\n",
    "        data = [x.strip().split(\" \") for x in data]\n",
    "        data = [[int(x) for x in y] for y in data]\n",
    "\n",
    "        return data\n",
    "\n",
    "    def _load_data(self):\n",
    "        self.source = self._load_label_data(self.source_label_path)\n",
    "        self.target = self._load_label_data(self.target_label_path)\n",
    "        self.source_indices = self._load_index_data(self.source_indices_path)\n",
    "        self.target_indices = self._load_index_data(self.target_indices_path)\n",
    "\n",
    "        assert len(self.source) == len(self.target)\n",
    "        return None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.source)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        source = self.source[idx]\n",
    "        target = self.target[idx]\n",
    "\n",
    "        source_indices = self.source_indices[idx]\n",
    "        target_indices = self.target_indices[idx]\n",
    "\n",
    "        # Convert to tensors\n",
    "        source_indices = torch.tensor(source_indices).long()\n",
    "        target_indices = torch.tensor(target_indices).long()\n",
    "\n",
    "        return source_indices, target_indices, source, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    We receive a list of tuples 4 long.\n",
    "    Each tuple is a tokenized name, a tokenized country (which is one-hot),\n",
    "    the name, and the country as strings.\n",
    "\n",
    "    We want to pad and stack them.\n",
    "    \"\"\"\n",
    "\n",
    "    x = [item[0] for item in batch]\n",
    "    y = [item[1] for item in batch]\n",
    "    source = [item[2] for item in batch]\n",
    "    target = [item[3] for item in batch]\n",
    "\n",
    "    # Lengths to pass the pack and pad sequence function\n",
    "    x_len = [len(item) for item in x]\n",
    "    y_len = [len(item) for item in y]\n",
    "\n",
    "    # Pad the sequences\n",
    "    x = torch.nn.utils.rnn.pad_sequence(x, batch_first=True)\n",
    "    y = torch.nn.utils.rnn.pad_sequence(y, batch_first=True)\n",
    "\n",
    "    # Now since we're teacher forcing we need two versions of y. One with\n",
    "    # missing the start token, y_target, and one with missing the end token,\n",
    "    # y_input\n",
    "    y_target = y[:, 1:]\n",
    "    y_input = y[:, :-1]\n",
    "\n",
    "    return x, y_input, y_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = LanguageTranslationDataset(\n",
    "    source_label_path=ENGLISH_INPUT_PATH,\n",
    "    target_label_path=FRENCH_INPUT_PATH,\n",
    "    source_indices_path=ENGLISH_INDICES_INPUT_PATH,\n",
    "    target_indices_path=FRENCH_INDICES_INPUT_PATH,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  tensor([   2,   18,   76,  528,    7, 3379,    7,  109,    4,    3])\n",
      "Target:  tensor([   2,   26,   16,  656,    6,  490,  250,   25, 1036,    4,    3])\n",
      "source:  Tom has decided to propose to Mary.\n",
      "target:  Tom a décidé de demander Marie en mariage.\n",
      "\n",
      "Input:  ['<SOS>', 'Tom', 'has', 'decided', 'to', 'propose', 'to', 'Mary', '.', '<EOS>']\n",
      "Target:  ['<SOS>', 'Tom', 'a', 'décidé', 'de', 'demander', 'Marie', 'en', 'mariage', '.', '<EOS>']\n"
     ]
    }
   ],
   "source": [
    "# Inspect a random sample\n",
    "# Show random example\n",
    "x, y, source, target = dataset[np.random.randint(0, len(dataset))]\n",
    "print(\"Input: \", x)\n",
    "print(\"Target: \", y)\n",
    "print(\"source: \", source)\n",
    "print(\"target: \", target)\n",
    "\n",
    "# And our language model can invert the indices\n",
    "print(\"\\nInput: \", source_language.index_to_token(x))\n",
    "print(\"Target: \", target_language.index_to_token(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val = torch.utils.data.random_split(dataset, [0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a dataloader that pulls 1 batch at a time. Note that more than 1 batch\n",
    "# will throw an error since we have variable length sequences. We'd need to pass\n",
    "# a custom collation function for that, which we'll do in the next notebook.\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    collate_fn=custom_collate_fn,\n",
    ")\n",
    "\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    val,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    collate_fn=custom_collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape:  torch.Size([128, 19])\n",
      "Y input shape:  torch.Size([128, 18])\n",
      "Y target shape:  torch.Size([128, 18])\n",
      "\n",
      "Decode the first row\n",
      "X:  ['<SOS>', 'I', 'met', 'a', 'friend', 'while', 'I', 'was', 'waiting', 'for', 'a', 'bus', '.', '<EOS>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "Y input:  ['<SOS>', \"J'\", 'ai', 'rencontré', 'un', 'ami', 'tandis', 'que', \"j'\", 'attendais', 'un', 'bus', '.', '<EOS>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "Y target:  [\"J'\", 'ai', 'rencontré', 'un', 'ami', 'tandis', 'que', \"j'\", 'attendais', 'un', 'bus', '.', '<EOS>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n"
     ]
    }
   ],
   "source": [
    "# Inspect the data loader.\n",
    "\n",
    "x, y_input, y_target = next(iter(train_dataloader))\n",
    "print(\"X shape: \", x.shape)\n",
    "print(\"Y input shape: \", y_input.shape)\n",
    "print(\"Y target shape: \", y_target.shape)\n",
    "\n",
    "# Just to be sure, decode the first row\n",
    "print(\"\\nDecode the first row\")\n",
    "print(\"X: \", source_language.index_to_token(x[0]))\n",
    "print(\"Y input: \", target_language.index_to_token(y_input[0]))\n",
    "print(\"Y target: \", target_language.index_to_token(y_target[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(L.LightningModule):\n",
    "    def __init__(\n",
    "        self, input_vocab_len, output_vocab_len, embedding_size, hidden_size\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_vocab_len = input_vocab_len\n",
    "        self.output_vocab_len = output_vocab_len\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.criterion = torch.nn.NLLLoss()\n",
    "\n",
    "        # Embedding\n",
    "        self.input_embedding = torch.nn.Embedding(\n",
    "            self.input_vocab_len, self.embedding_size\n",
    "        )\n",
    "        self.output_embedding = torch.nn.Embedding(\n",
    "            self.output_vocab_len, self.embedding_size\n",
    "        )\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = torch.nn.GRU(\n",
    "            self.embedding_size,\n",
    "            self.hidden_size,\n",
    "            batch_first=True,\n",
    "            dropout=0.2,\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        # The input to the decoder will be a concat of the\n",
    "        self.decoder = torch.nn.GRU(\n",
    "            self.embedding_size + self.hidden_size,\n",
    "            self.hidden_size,\n",
    "            batch_first=True,\n",
    "            dropout=0.2,\n",
    "        )\n",
    "\n",
    "        self.dense = torch.nn.Linear(self.hidden_size, self.output_vocab_len)\n",
    "\n",
    "        self.log_softmax = torch.nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def encoder_step(self, x):\n",
    "        \"\"\"\n",
    "        Push inputs through encoder.\n",
    "        \"\"\"\n",
    "\n",
    "        # Shapes are:\n",
    "        # x: (batch_size, seq_len)\n",
    "        # decoder_input: (batch_size, seq_len-1)\n",
    "        # decoder_target: (batch_size, seq_len-1)\n",
    "\n",
    "        # Construct embeddings\n",
    "        x = self.input_embedding(x)\n",
    "        # x: (batch_size, seq_len, embedding_size)\n",
    "\n",
    "        # Run the encoder\n",
    "        encoder_output, context_vector = self.encoder(x)\n",
    "        # output: (batch_size, seq_len, hidden_size)\n",
    "        # context_vector: (1, batch_size, hidden_size)\n",
    "\n",
    "        return context_vector\n",
    "\n",
    "    def decoder_step(self, decoder_input, context_vector, decoder_state=None):\n",
    "        \"\"\"\n",
    "        Push inputs through decoder.\n",
    "        \"\"\"\n",
    "\n",
    "        # Embed target outputs\n",
    "        decoder_input = self.output_embedding(decoder_input)\n",
    "        # decoder_input: (batch_size, seq_len-1, embedding_size)\n",
    "\n",
    "        # Currently the context vector is (1, batch_size, hidden_size)\n",
    "        # decoder_input is (batch_size, seq_len-1, embedding_size)\n",
    "        # Rather than construct a special RNN module that can handle two inputs,\n",
    "        # we're simply going to concatenate the context vector to the decoder input.\n",
    "\n",
    "        # Permute the dimensions of context vector to be conformable with decoder_input\n",
    "        context_vector = context_vector.permute(1, 0, 2)\n",
    "        # context_vector: (batch_size, 1, hidden_size)\n",
    "\n",
    "        # Make copies of the context vector along the sequence length demiension\n",
    "        context_vector = context_vector.repeat(1, decoder_input.shape[1], 1)\n",
    "        # context_vector: (batch_size, seq_len-1, hidden_size)\n",
    "\n",
    "        decoder_input = torch.cat([decoder_input, context_vector], dim=2)\n",
    "        # decoder_input: (batch_size, seq_len-1, embedding_size + hidden_size)\n",
    "\n",
    "        # Now run the decoder_input through the decoder\n",
    "        decoder_output, decoder_state = self.decoder(\n",
    "            decoder_input, decoder_state\n",
    "        )\n",
    "        # decoder_output: (batch_size, seq_len-1, hidden_size)\n",
    "\n",
    "        # decoder_output is (batch_size, seq_len-1, hidden_size)\n",
    "        # Add a dense layer to convert it to the decoder_output vocab size\n",
    "        decoder_output = self.dense(decoder_output)\n",
    "        # decoder_output: (batch_size, seq_len-1, decoder_output_vocab_size)\n",
    "\n",
    "        # Now log softmax the decoder_output along last dimension\n",
    "        decoder_output = self.log_softmax(decoder_output)\n",
    "        # output: (batch_size, seq_len-1, output_vocab_size)\n",
    "\n",
    "        return decoder_output, decoder_state\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, decoder_input, decoder_target = batch\n",
    "\n",
    "        # Get context vector\n",
    "        context_vector = self.encoder_step(x)\n",
    "\n",
    "        # Get decoder output\n",
    "        decoder_output, _ = self.decoder_step(decoder_input, context_vector)\n",
    "        # decoder_output: (batch_size, seq_len-1, output_vocab_size)\n",
    "\n",
    "        # To use our NLL loss, we need to reshape the output and target\n",
    "        # The outputs need to be (N, class_size) and the targets (N)\n",
    "        # So flatten the batch and sequence dimensions.\n",
    "        decoder_output = decoder_output.reshape(-1, self.output_vocab_len)\n",
    "        decoder_target = decoder_target.reshape(-1)\n",
    "\n",
    "        # output: (batch_size * seq_len-1, output_vocab_size)\n",
    "        # decoder_target: (batch_size * seq_len-1, )\n",
    "\n",
    "        loss = self.criterion(decoder_output, decoder_target)\n",
    "\n",
    "        self.log(\"train_loss\", loss)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Same as training step. We have the option to add more metrics here.\n",
    "        \"\"\"\n",
    "        x, decoder_input, decoder_target = batch\n",
    "        context_vector = self.encoder_step(x)\n",
    "\n",
    "        decoder_output, _ = self.decoder_step(decoder_input, context_vector)\n",
    "        decoder_output = decoder_output.reshape(-1, self.output_vocab_len)\n",
    "        decoder_target = decoder_target.reshape(-1)\n",
    "\n",
    "        loss = self.criterion(decoder_output, decoder_target)\n",
    "\n",
    "        self.log(\"validation_loss\", loss)\n",
    "\n",
    "        return None\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "\n",
    "        # One cycle learning rate scheduler\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer,\n",
    "            max_lr=0.01,\n",
    "            steps_per_epoch=len(train_dataloader),\n",
    "            epochs=EPOCHS,\n",
    "        )\n",
    "\n",
    "        # # Reduce on plateau\n",
    "        # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        #     optimizer,\n",
    "        #     mode=\"min\",\n",
    "        #     factor=0.5,\n",
    "        #     patience=3,\n",
    "        #     verbose=True,\n",
    "        # )\n",
    "\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": scheduler,\n",
    "            \"monitor\": \"validation_loss\",\n",
    "        }\n",
    "\n",
    "    def inference(self, name: str):\n",
    "        \"\"\"\n",
    "        Run inference on a single example.\n",
    "        \"\"\"\n",
    "        assert isinstance(name, str)\n",
    "\n",
    "        # Tokenize the input\n",
    "        input_tokens = source_language.tokenizer(name)\n",
    "\n",
    "        # Add <SOS> and <EOS> tokens\n",
    "        input_tokens = [\"<SOS>\"] + input_tokens + [\"<EOS>\"]\n",
    "\n",
    "        input_indices = source_language.token_to_index(input_tokens)\n",
    "\n",
    "        x = torch.tensor(input_indices).unsqueeze(0).long()\n",
    "        # x should have shape (1, seq_len)\n",
    "        assert x.shape[0] == 1\n",
    "\n",
    "        # Get context vector\n",
    "        context_vector = self.encoder_step(x)\n",
    "\n",
    "        # Now we need to run the decoder\n",
    "        # We'll start with a start token and no state\n",
    "        decoder_input = torch.tensor(\n",
    "            [target_language.token_to_index(\"<SOS>\")]\n",
    "        ).unsqueeze(0)\n",
    "        decoder_state = None\n",
    "        # decoder_input: (1, 1) and it's a long\n",
    "\n",
    "        # We'll keep track of the output\n",
    "        output = []\n",
    "\n",
    "        reached_eos = False\n",
    "        for i in range(10):\n",
    "            # Run the decoder\n",
    "            decoder_output, decoder_state = self.decoder_step(\n",
    "                decoder_input, context_vector, decoder_state\n",
    "            )\n",
    "            # decoder_output: (1, 1, output_vocab_size)\n",
    "\n",
    "            # Get the most likely token\n",
    "            token = torch.argmax(decoder_output, dim=-1)\n",
    "            # token: (1, 1)\n",
    "\n",
    "            # Append to the output\n",
    "            output.append(token.squeeze(0).item())\n",
    "\n",
    "            # If we've reached the end of the sentence, break\n",
    "            if token.item() == target_language.token_to_index(\"<EOS>\"):\n",
    "                reached_eos = True\n",
    "                break\n",
    "\n",
    "            # Update the decoder input\n",
    "            decoder_input = token\n",
    "\n",
    "        if not reached_eos:\n",
    "            logger.info(\"Failed to reach EOS token\")\n",
    "\n",
    "        # Convert the output to a string\n",
    "        output = [target_language.index_to_token(x) for x in output]\n",
    "        # If the last token is <EOS> remove it\n",
    "        if output[-1] == \"<EOS>\":\n",
    "            output = output[:-1]\n",
    "\n",
    "        output = \" \".join(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EncoderDecoder(\n",
    "    input_vocab_len=SOURCE_VOCAB_SIZE,\n",
    "    output_vocab_len=TARGET_VOCAB_SIZE,\n",
    "    embedding_size=EMBEDDING_SIZE,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    ")\n",
    "\n",
    "cwd = os.getcwd()\n",
    "parent_dir = os.path.dirname(cwd)\n",
    "\n",
    "tensorboard_logger = TensorBoardLogger(\n",
    "    save_dir=parent_dir,\n",
    "    name=\"logs/english_french_encoder_decoder\",\n",
    "    version=f\"UNIT={UNIT_TYPE}_BATCH_SIZE={BATCH_SIZE}_EMBEDDING_SIZE={EMBEDDING_SIZE}_HIDDEN_SIZE={HIDDEN_SIZE}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "trainer = L.Trainer(\n",
    "    devices=1,\n",
    "    accelerator=ACCELERATOR,\n",
    "    logger=tensorboard_logger,\n",
    "    max_epochs=EPOCHS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name             | Type       | Params | Mode \n",
      "--------------------------------------------------------\n",
      "0 | criterion        | NLLLoss    | 0      | train\n",
      "1 | input_embedding  | Embedding  | 960 K  | train\n",
      "2 | output_embedding | Embedding  | 960 K  | train\n",
      "3 | encoder          | GRU        | 296 K  | train\n",
      "4 | decoder          | GRU        | 493 K  | train\n",
      "5 | dense            | Linear     | 1.9 M  | train\n",
      "6 | log_softmax      | LogSoftmax | 0      | train\n",
      "--------------------------------------------------------\n",
      "4.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.6 M     Total params\n",
      "18.556    Total estimated model params size (MB)\n",
      "7         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|██████████| 743/743 [00:13<00:00, 54.61it/s, v_num==256]    "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|██████████| 743/743 [00:13<00:00, 53.54it/s, v_num==256]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, train_dataloader, val_dataloader)\n",
    "model.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "if not os.path.exists(\"../models\"):\n",
    "    os.makedirs(\"../models\")\n",
    "\n",
    "# torch.save(model.state_dict(), \"../models/english_french_encoder_decoder.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rob/encoder-decoder/.venv/lib/python3.12/site-packages/torch/nn/modules/rnn.py:88: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "/tmp/ipykernel_219083/3753899557.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"../models/english_french_encoder_decoder.pth\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model\n",
    "model = EncoderDecoder(\n",
    "    input_vocab_len=SOURCE_VOCAB_SIZE,\n",
    "    output_vocab_len=TARGET_VOCAB_SIZE,\n",
    "    embedding_size=EMBEDDING_SIZE,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    ")\n",
    "\n",
    "model.load_state_dict(\n",
    "    torch.load(\"../models/english_french_encoder_decoder.pth\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bonjour , comment allez -vous \\u202f ?'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Hello, how are you?\"\n",
    "model.inference(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source: \t I've already apologized for that.\n",
      "target: \t J'ai déjà présenté mes excuses pour cela.\n",
      "Predicted: \t J' ai déjà présenté mes excuses pour ça .\n",
      "\n",
      "\n",
      "source: \t The army had plenty of weapons.\n",
      "target: \t L'armée disposait de tas d'armes.\n",
      "Predicted: \t L' armée disposait de tas d' armes .\n",
      "\n",
      "\n",
      "source: \t You're forgiven.\n",
      "target: \t Vous êtes pardonnées.\n",
      "Predicted: \t Vous êtes <UNK> .\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run it over the dataset\n",
    "\n",
    "for i in range(3):\n",
    "    x, y, source, target = dataset[np.random.randint(0, len(dataset))]\n",
    "    print(\"source: \\t\", source)\n",
    "    print(\"target: \\t\", target)\n",
    "    print(\"Predicted: \\t\", model.inference(source))\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
