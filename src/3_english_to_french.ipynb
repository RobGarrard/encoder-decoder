{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# English-to-French Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, this is a harder task and a bigger data set. It'll take a little while longer to train this one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import yaml\n",
    "\n",
    "import torch\n",
    "import lightning as L\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from pytorch_lightning import seed_everything\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from common.language import load_language\n",
    "\n",
    "from common.dataloaders import (\n",
    "    TranslationDataset,\n",
    "    padding_collator,\n",
    ")\n",
    "\n",
    "from common.models import (\n",
    "    EncoderDecoder,\n",
    ")\n",
    "\n",
    "from common.utils import (\n",
    "    get_logger,\n",
    "    Timer,\n",
    ")\n",
    "\n",
    "logger = get_logger(\"english-to-french-translation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-14 21:01:28 - english-to-french-translation - INFO: Current working directory: /home/rob/encoder-decoder\n"
     ]
    }
   ],
   "source": [
    "seed_everything(2718)\n",
    "\n",
    "# Set the cwd to the root of the project.\n",
    "# Only let this execute once\n",
    "if os.getcwd().endswith(\"src\"):\n",
    "    os.chdir(\"..\")\n",
    "\n",
    "logger.info(f\"Current working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config.yaml. This contains all of our paths and constants.\n",
    "with open(\"config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Training params\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 128\n",
    "EMBEDDING_SIZE = 512\n",
    "HIDDEN_SIZE = 512\n",
    "NUM_LAYERS = 1\n",
    "DROPOUT = 0.0 # Dropout is between rnn layers\n",
    "\n",
    "# No unit type option here, just using GRUs.\n",
    "\n",
    "# Trainer params\n",
    "ACCELERATOR = \"gpu\"  # \"cpu\" or \"gpu\"\n",
    "\n",
    "# CPUS to give each dataloader\n",
    "NUM_WORKERS = 3\n",
    "\n",
    "# Every time you run training, the logs will have this tag attached.\n",
    "# If you rerun with the same tag, the log will be overwritten.\n",
    "TAG = (\n",
    "    f\"en-to-fr_\"\n",
    "    f\"BATCH_SIZE={BATCH_SIZE}_\"\n",
    "    f\"EMBEDDING_SIZE={EMBEDDING_SIZE}_\"\n",
    "    f\"HIDDEN_SIZE={HIDDEN_SIZE}_\"\n",
    "    f\"LAYERS={NUM_LAYERS}_\"\n",
    "    f\"DROPOUT={DROPOUT}_\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_language = load_language(config[\"ENGLISH_LANGUAGE_MODEL_PATH\"])\n",
    "french_language = load_language(config[\"FRENCH_LANGUAGE_MODEL_PATH\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Names vocab size: 10004\n",
      "Countries vocab size: 10004\n",
      "Top 10 most common tokens in names vocabulary:\n",
      "<PAD>: 0\n",
      "<UNK>: 0\n",
      "<SOS>: 0\n",
      "<EOS>: 0\n",
      ".: 113072\n",
      "I: 43225\n",
      "you: 29605\n",
      "to: 27343\n",
      "?: 21962\n",
      "the: 20530\n",
      "\n",
      "Top 10 most common tokens in countries vocabulary:\n",
      "<PAD>: 0\n",
      "<UNK>: 0\n",
      "<SOS>: 0\n",
      "<EOS>: 0\n",
      ".: 109003\n",
      "Je: 26561\n",
      "de: 26338\n",
      "?: 21961\n",
      "pas: 21390\n",
      "est: 19385\n"
     ]
    }
   ],
   "source": [
    "NAMES_VOCAB_SIZE = len(english_language.vocabulary)\n",
    "COUNTRIES_VOCAB_SIZE = len(french_language.vocabulary)\n",
    "\n",
    "print(f\"Names vocab size: {NAMES_VOCAB_SIZE}\")\n",
    "print(f\"Countries vocab size: {COUNTRIES_VOCAB_SIZE}\")\n",
    "\n",
    "# Vocabulary attributes are a dictionary with the token being the\n",
    "# key and the index being how frequently the token appears in the corpus.\n",
    "# Note that since we've added the special tokens ourselves, they will\n",
    "# have frequency 0.\n",
    "name_vocab = list(english_language.vocabulary.keys())\n",
    "country_vocab = list(french_language.vocabulary.keys())\n",
    "\n",
    "print(\"Top 10 most common tokens in names vocabulary:\")\n",
    "for i in range(10):\n",
    "    print(f\"{name_vocab[i]}: {english_language.vocabulary[name_vocab[i]]}\")\n",
    "\n",
    "print(\"\\nTop 10 most common tokens in countries vocabulary:\")\n",
    "for i in range(10):\n",
    "    print(\n",
    "        f\"{country_vocab[i]}: {french_language.vocabulary[country_vocab[i]]}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TranslationDataset(\n",
    "    source_labels_path=config[\"ENGLISH_INPUT_PATH\"],\n",
    "    target_labels_path=config[\"FRENCH_INPUT_PATH\"],\n",
    "    source_indices_path=config[\"ENGLISH_OUTPUT_PATH\"],\n",
    "    target_indices_path=config[\"FRENCH_OUTPUT_PATH\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  tensor([   2,   18,   76,  528,    7, 3379,    7,  109,    4,    3])\n",
      "Target:  tensor([   2,   26,   16,  656,    6,  490,  250,   25, 1036,    4,    3])\n",
      "english:  Tom has decided to propose to Mary.\n",
      "french:  Tom a décidé de demander Marie en mariage.\n"
     ]
    }
   ],
   "source": [
    "# Show random example\n",
    "x, y, english, french = dataset[np.random.randint(0, len(dataset))]\n",
    "print(\"Input: \", x)\n",
    "print(\"Target: \", y)\n",
    "print(\"english: \", english)\n",
    "print(\"french: \", french)\n",
    "\n",
    "# Note that both source and target languages have <SOS> and  <EOS> tokens now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data = torch.utils.data.random_split(dataset, [0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataloaders\n",
    "# We use a collate function to pad the sequences to the same length.\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    collate_fn=padding_collator,\n",
    ")\n",
    "\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    val_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    collate_fn=padding_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source shape:  torch.Size([128, 19])\n",
      "Target shape:  torch.Size([128, 20])\n",
      "\n",
      "Detokenize the first row\n",
      "Source:  ['<SOS>', 'I', \"'ll\", 'be', 'there', 'tomorrow', '.', '<EOS>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "Target:  ['<SOS>', 'Je', 'serai', 'là', '-', 'bas', 'demain', '.', '<EOS>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n"
     ]
    }
   ],
   "source": [
    "# Inspect the data loader.\n",
    "\n",
    "x, y, english, french = next(iter(train_dataloader))\n",
    "print(\"Source shape: \", x.shape)\n",
    "print(\"Target shape: \", y.shape)\n",
    "\n",
    "\n",
    "# Just to be sure, detokenize the first row\n",
    "print(\"\\nDetokenize the first row\")\n",
    "print(\"Source: \", english_language.index_to_token(x[0]))\n",
    "print(\"Target: \", french_language.index_to_token(y[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EncoderDecoder(\n",
    "    source_language=english_language,\n",
    "    target_language=french_language,\n",
    "    detokenizer=lambda s: \" \".join(s),\n",
    "    input_size=NAMES_VOCAB_SIZE,\n",
    "    output_size=COUNTRIES_VOCAB_SIZE,\n",
    "    embedding_size=EMBEDDING_SIZE,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dropout=DROPOUT,\n",
    "    epochs=EPOCHS,\n",
    "    data_length=len(train_data),\n",
    "    max_output_length=30,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context vector shape: torch.Size([1, 128, 512])\n",
      "Output shape: torch.Size([128, 19, 10004])\n",
      "Hidden shape: torch.Size([1, 128, 512])\n"
     ]
    }
   ],
   "source": [
    "# Check that we can forward pass with the x,y generated above.\n",
    "\n",
    "context_vector = model.encoder_step(x)\n",
    "print(f\"Context vector shape: {context_vector.shape}\")\n",
    "\n",
    "output, hidden = model.decoder_step(y[:, :-1], context_vector, context_vector)\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Hidden shape: {hidden.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has 19,576,596 parameters.\n"
     ]
    }
   ],
   "source": [
    "# Print how many parameters the model has\n",
    "print(\n",
    "    f\"Model has {sum(p.numel() for p in model.parameters() if p.requires_grad):,} parameters.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so that's a little bigger than previously. Most of these are in the dense layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training logs\n",
    "\n",
    "tensorboard_logger = TensorBoardLogger(\n",
    "    save_dir=config[\"TENSORBOARD_LOGS_PATH\"],\n",
    "    name=\"english-to-french-translation/\",\n",
    "    version=TAG,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use our nominated accelerator and log to tensorboard\n",
    "trainer = L.Trainer(\n",
    "    devices=1,\n",
    "    accelerator=ACCELERATOR,\n",
    "    logger=tensorboard_logger,\n",
    "    max_epochs=EPOCHS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rob/encoder-decoder/.venv/lib/python3.12/site-packages/lightning/pytorch/utilities/data.py:78: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 128. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 850/850 [00:46<00:00, 18.19it/s, v_num=0.0_]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rob/encoder-decoder/.venv/lib/python3.12/site-packages/lightning/pytorch/utilities/data.py:78: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 32. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 850/850 [00:47<00:00, 17.72it/s, v_num=0.0_]\n",
      "Elapsed time: 00:00:48\n"
     ]
    }
   ],
   "source": [
    "timer = Timer()\n",
    "trainer.fit(model, train_dataloader, val_dataloader)\n",
    "print(f\"Elapsed time: {timer.toc()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), config[\"ENGLISH_TO_FRENCH_TRANSLATION_MODEL_PATH\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Do you speak English?\n",
      "Output: Combien de temps -vous ?\n",
      "\n",
      "Input: Where is the library?\n",
      "Output: Quel est le <UNK>   ?\n",
      "\n",
      "Input: Such is life!\n",
      "Output: C' est un peu de <UNK> .\n",
      "\n",
      "Input: Where is my wine and my cheese? Do you have it?\n",
      "Output: Combien de temps est -ce que tu as fait de la maison   ?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.freeze()\n",
    "\n",
    "english_sentences = [\n",
    "    \"Do you speak English?\",\n",
    "    \"Where is the library?\",\n",
    "    \"Such is life!\",\n",
    "    \"Where is my wine and my cheese? Do you have it?\",\n",
    "]\n",
    "\n",
    "for sentence in english_sentences:\n",
    "    translated = model.inference(sentence)\n",
    "    print(f\"Input: {sentence}\")\n",
    "    print(f\"Output: {translated}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
